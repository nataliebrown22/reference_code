{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holds functions for pre-processing and machine learning models\n",
    "**Author:** Natalie Brown \n",
    "\n",
    "\n",
    "**Contents:**\n",
    "* **Pre-processing**\n",
    "    * normalization techniques\n",
    "    * OneHot Encoding\n",
    "* **Feature Selection**\n",
    "    * PCA (principal component analysis)\n",
    "* **Machine Learning**\n",
    "    * linear regression\n",
    "    * logistic regression\n",
    "    * decision trees\n",
    "    * extreme gradient boosting\n",
    "* **Evaluation Metrics**\n",
    "    * linear metrics\n",
    "    * classification metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, Normalizer # transform / normalize\n",
    "from sklearn.preprocessing import OneHotEncoder # encode categorical features\n",
    "\n",
    "\n",
    "# feature selection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split # split data\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression # linear\n",
    "from sklearn.tree import DecisionTreeClassifier # tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier # ensemble\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, mean_squared_error, r2_score # evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads data\n",
    "def load_data(file_name, file_type, sheet_name=None):\n",
    "    if file_type == 'csv':\n",
    "        df = pd.read_csv(file_name, sep='delimeter', encoding='utf-8')\n",
    "\n",
    "    elif file_type == 'excel':\n",
    "        df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "\n",
    "    else:\n",
    "        print(f'Enter file type\\n-csv\\n-excel')\n",
    "        df = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "takes a data frame, a list with column(s) to normalize, and the normalization method as input\n",
    "returns the original dataframe merged with the new normalized columns\n",
    "normalized columns are denoted by NORM\n",
    "\n",
    "method should be one of these:\n",
    "- NORM (normalization)\n",
    "- LN (natural log transformation)\n",
    "- SCALE (z-score standardization)\n",
    "'''\n",
    "\n",
    "def normalize_features(df, cols_to_norm, method):\n",
    "    \n",
    "    if method == 'SCALE':\n",
    "        \n",
    "        # initialize scaler \n",
    "        sc = StandardScaler()\n",
    "    \n",
    "        # apply and create new columns\n",
    "        prepro_data = sc.fit_transform(df[cols_to_norm])\n",
    "        prepro_df = pd.DataFrame(prepro_data, columns=[col + '_SCALED' for col in cols_to_norm])\n",
    "\n",
    "        # merge\n",
    "        output_df = pd.concat([df, prepro_df], axis=1)\n",
    "\n",
    "    elif method == 'LN':\n",
    "\n",
    "        # intialize Log Transformer\n",
    "        ln = FunctionTransformer(np.log, validate=True)\n",
    "\n",
    "        # apply and create new columns\n",
    "        prepro_data = ln.fit_transform(df[cols_to_norm])\n",
    "        prepro_df = pd.DataFrame(prepro_data, columns=[col + '_LN' for col in cols_to_norm])\n",
    "\n",
    "        # merge\n",
    "        output_df = pd.concat([df, prepro_df], axis=1)\n",
    "\n",
    "    elif method == 'NORM':\n",
    "\n",
    "        # intialize normalization\n",
    "        nm = Normalizer()\n",
    "\n",
    "        # apply and create new columns\n",
    "        prepro_data = nm.fit_transform(df[cols_to_norm])\n",
    "        prepro_df = pd.DataFrame(prepro_data, columns=[col + '_NORM' for col in cols_to_norm])\n",
    "\n",
    "        # merge\n",
    "        output_df = pd.concat([df, prepro_df], axis=1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('no / incorrect method chosen\\nenter:- NORM\\n-LN\\n-SCALE')\n",
    "\n",
    "        output_df = df.copy()\n",
    "        \n",
    "    # Combine the original DataFrame with the scaled DataFrame\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### categorical encoding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "takes the data frame, features to be encoded\n",
    "returns data frame with the original features dropped, replaced with the new dummies\n",
    "'''\n",
    "\n",
    "def encode_categorical(df, cols_to_encode):\n",
    "\n",
    "    # intitialize encoder\n",
    "    encoder = OneHotEncoder(drop='if_binary', sparse=False)\n",
    "\n",
    "    # apply encoder\n",
    "    encoded_data = encoder.fit_transform(df[cols_to_encode])\n",
    "\n",
    "    # create data frame from encoded data\n",
    "    encoded_columns = encoder.get_feature_names_out(cols_to_encode)\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns, index=df.index)\n",
    "    \n",
    "    # Drop original columns from the DataFrame and merge the encoded DataFrame\n",
    "    return pd.concat([df.drop(columns=cols_to_encode), encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### principle component analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to perform pca, which helps determine which features ot use, especially helpful with highly correlated data\n",
    "takes data frame and number of principal components as input\n",
    "returns a dataframe with the PCs\n",
    "'''\n",
    "\n",
    "def pca(df, n_components):\n",
    "\n",
    "    # intialize\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # fit to data\n",
    "    pca.fit(df)\n",
    "\n",
    "    # get pca data\n",
    "    pca_data = pca.transform(df)\n",
    "\n",
    "\n",
    "    columns = [f'PC{n + 1}' for n in range(n_components)]\n",
    "\n",
    "\n",
    "    # put results into a dataframe\n",
    "    pca_df = pd.DataFrame(pca_data, columns=columns)\n",
    "\n",
    "    # get loadings\n",
    "    loadings = pca.components_\n",
    "    \n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=df.columns, # names from the original DataFrame\n",
    "        index=[f'PC{i+1}' for i in range(pca.components_.shape[0])]  # Names for each principal component\n",
    "    )\n",
    "    \n",
    "    for pc in loadings.index:\n",
    "        print(f\"\\n{pc}:\")\n",
    "        # Sort features by their absolute contribution to the current PC\n",
    "        top_features = loadings.loc[pc].abs().sort_values(ascending=False).head(4)\n",
    "        for feature, loading in zip(top_features.index, top_features.values):\n",
    "            print(f\"Feature: {feature}, Loading: {loading:.4f}\")\n",
    "\n",
    "    # return the data frame\n",
    "    return pca_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### linear regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model\n",
    "def linear_regression(df,target,train_size):\n",
    "    # define target and inputs\n",
    "    X = df.drop(columns=[target]) # inputs\n",
    "    y = df[target] # target\n",
    "\n",
    "    # split the data intro the training set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_size))\n",
    "    \n",
    "    # split the remaining 30% into validation and test set\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=.5)\n",
    "    \n",
    "    # sanity check for training size\n",
    "    training_size = (X_train.shape[0] / df.shape[0]) * 100\n",
    "    test_size = (X_test.shape[0] / df.shape[0]) * 100\n",
    "    validation_size = (X_val.shape[0] / df.shape[0]) * 100\n",
    "\n",
    "    print(f'''\n",
    "    Training: {training_size:.2f}%\n",
    "    Test: {test_size:.2f}%\n",
    "    Validation: {validation_size:.2f}%\\n\\n''')\n",
    "    \n",
    "    # initialize model and train\n",
    "    lr = LinearRegression() # intialize\n",
    "\n",
    "    return X_val, X_test, y_val, y_test, X_train, y_train, lr.fit(X_train, y_train) # train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### logistic regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for logistic regression model\n",
    "def logistic_regression(df,target,train_size,test_size):\n",
    "    # define target and inputs\n",
    "    X = df.drop(columns=[target]) # inputs\n",
    "    y = df[target] # target\n",
    "    \n",
    "    # split the data intro the training set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_size))\n",
    "\n",
    "    # determine the test size proportion\n",
    "    n = test_size / (1 - train_size)\n",
    "    \n",
    "    # split the remaining 30% into validation and test set\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=n)\n",
    "    \n",
    "    # sanity check for training size\n",
    "    training_size = (X_train.shape[0] / df.shape[0]) * 100\n",
    "    test_size = (X_test.shape[0] / df.shape[0]) * 100\n",
    "    validation_size = (X_val.shape[0] / df.shape[0]) * 100\n",
    "    \n",
    "    print(f'''\n",
    "    Training: {training_size:.2f}%\n",
    "    Test: {test_size:.2f}%\n",
    "    Validation: {validation_size:.2f}%\\n\\n''')\n",
    "    \n",
    "    # initialize model and train\n",
    "    lg = LogisticRegression()\n",
    "\n",
    "    return X_val, X_test, y_val, y_test, X_train, y_train, lg.fit(X_train, y_train) # train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### decision tree\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **DecisionTreeClassifier**(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(df,target,train_size, depth, minimum_splits, minimum_leaves):\n",
    "    # define target and inputs\n",
    "    X = df.drop(columns=[target]) # inputs\n",
    "    y = df[target] # target\n",
    "\n",
    "    # split the data intro the training set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_size))\n",
    "    \n",
    "    # split the remaining 30% into validation and test set\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=.5)\n",
    "    \n",
    "    # sanity check for training size\n",
    "    training_size = (X_train.shape[0] / df.shape[0]) * 100\n",
    "    test_size = (X_test.shape[0] / df.shape[0]) * 100\n",
    "    validation_size = (X_val.shape[0] / df.shape[0]) * 100\n",
    "    \n",
    "    print(f'''\n",
    "    Training: {training_size:.2f}%\n",
    "    Test: {test_size:.2f}%\n",
    "    Validation: {validation_size:.2f}%\\n\\n''')\n",
    "    \n",
    "    # initialize model and train\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, min_samples_split=minimum_splits, min_samples_leaf=minimum_leaves) # intialize\n",
    "    \n",
    "    return X_val, X_test, y_val, y_test, X_train, y_train, dt.fit(X_train, y_train) # train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### gradient boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GradientBoostingClassifier**(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boost(df, target, train_size, estimators, learning_rate, max_depth): # learning rate should be a decimal\n",
    "    # define target and inputs\n",
    "    X = df.drop(columns=[target]) # inputs\n",
    "    y = df[target] # target\n",
    "\n",
    "    # split the data intro the training set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_size))\n",
    "    \n",
    "    # split the remaining 30% into validation and test set\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=.5)\n",
    "    \n",
    "    # sanity check for training size\n",
    "    training_size = (X_train.shape[0] / df.shape[0]) * 100\n",
    "    test_size = (X_test.shape[0] / df.shape[0]) * 100\n",
    "    validation_size = (X_val.shape[0] / df.shape[0]) * 100\n",
    "    \n",
    "    print(f'''\n",
    "    Training: {training_size:.2f}%\n",
    "    Test: {test_size:.2f}%\n",
    "    Validation: {validation_size:.2f}%\\n\\n''')\n",
    "    \n",
    "    # initialize model and train\n",
    "    xgb = GradientBoostingClassifier(n_estimators=estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=0) # intialize\n",
    "    \n",
    "    return X_val, X_test, y_val, y_test, X_train, y_train, xgb.fit(X_train, y_train) # train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Evalutation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### classification metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(X_val, X_test, y_val, y_test, X_train, y_train, model):\n",
    "    \n",
    "    # Training Evaluation\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]  # Get probabilities for ROC AUC calculation\n",
    "    \n",
    "    print(f\"{'-'*55}\\nTraining Evaluation:\\n\")\n",
    "    \n",
    "    # Precision, Recall, and F1-Score\n",
    "    train_report_dict = classification_report(y_train, y_train_pred, output_dict=True)\n",
    "    train_metrics = pd.DataFrame(train_report_dict).transpose()\n",
    "    train_metrics = train_metrics.loc[['0', '1'], ['precision', 'recall', 'f1-score']]  # Include only class labels (0 and 1)\n",
    "    print(train_metrics)\n",
    "    \n",
    "    # AUC\n",
    "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
    "    print(f\"\\nAUC (Area Under Curve): {train_auc:.4f}\")\n",
    "    \n",
    "    # Validation Evaluation\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'-'*55}\\nValidation Evaluation:\\n\")\n",
    "    \n",
    "    # Precision, Recall, and F1-Score\n",
    "    val_report_dict = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    val_metrics = pd.DataFrame(val_report_dict).transpose()\n",
    "    val_metrics = val_metrics.loc[['0', '1'], ['precision', 'recall', 'f1-score']]  # Include only class labels (0 and 1)\n",
    "    print(val_metrics)\n",
    "    \n",
    "    # AUC\n",
    "    val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    print(f\"\\nAUC (Area Under Curve): {val_auc:.4f}\")\n",
    "    \n",
    "    # Test Evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'-'*55}\\nTest Evaluation:\\n\")\n",
    "    \n",
    "    # Precision, Recall, and F1-Score\n",
    "    test_report_dict = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    test_metrics = pd.DataFrame(test_report_dict).transpose()\n",
    "    test_metrics = test_metrics.loc[['0', '1'], ['precision', 'recall', 'f1-score']]  # Include only class labels (0 and 1)\n",
    "    print(test_metrics)\n",
    "    \n",
    "    # AUC\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "    print(f\"\\nAUC (Area Under Curve): {test_auc:.4f}\\n\\n\")\n",
    "\n",
    "    # calculate ROC for each set\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, model.predict_proba(X_train)[:, 1])\n",
    "    train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "    \n",
    "    # Validation set\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_val, model.predict_proba(X_val)[:, 1])\n",
    "    val_auc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "    \n",
    "    # Test set\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    # plot ROC curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr_train, tpr_train, label=f\"Training Set (AUC = {train_auc:.4f})\", color='blue')\n",
    "    plt.plot(fpr_val, tpr_val, label=f\"Validation Set (AUC = {val_auc:.4f})\", color='orange')\n",
    "    plt.plot(fpr_test, tpr_test, label=f\"Test Set (AUC = {test_auc:.4f})\", color='green')\n",
    "    \n",
    "    # Add diagonal line for random guess\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### linear regression metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linear_regression(data, target, X_val, X_test, y_val, y_test, X_train, y_train, model):\n",
    "\n",
    "    for dataset, X, y in [(\"Training\", X_train, y_train), \n",
    "                          (\"Validation\", X_val, y_val), \n",
    "                          (\"Test\", X_test, y_test)]:\n",
    "        \n",
    "        print(f\"\\n{'-'*55}\\n{dataset} Evaluation:\\n\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X)\n",
    "    \n",
    "        # Metrics: MSE and R-squared\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"R-squared: {r2:.4f}\\n\\n\")\n",
    "\n",
    "    # Predictions\n",
    "    prediction = model.predict(data.drop(columns=[target]))\n",
    "\n",
    "    # Actual value\n",
    "    actual = data[target]\n",
    "    \n",
    "    # plotting residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # getting residuals\n",
    "    residuals = actual - prediction\n",
    "\n",
    "    # scatter plot\n",
    "    plt.scatter(prediction, residuals, color='green', alpha=0.6, edgecolor='black')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.grid(alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # plotting predicted vs actual\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(prediction, actual, color='skyblue', alpha=0.6, edgecolor='black')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)  # Diagonal line\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.grid(alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coefficients and feature importance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get coefficients\n",
    "def coefficients(df,target,model):\n",
    "    coefficients = model.coef_[0]  # LogisticRegression stores coefficients as an array\n",
    "    feature_names = df.drop(columns=[target]).columns  # Retrieve the feature names from the training data\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "    \n",
    "\n",
    "    # Sort coefficients by magnitude for better visualization\n",
    "    coefficients_df = coefficients_df.sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(coefficients_df['Feature'], coefficients_df['Coefficient'], color='skyblue')\n",
    "    plt.xlabel(\"Coefficient Value\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(f\"Feature Coefficients in {model_type} Model\")\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)  # Add a vertical line at x=0 for reference\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return coefficients_df.sort_values(by=['Coefficient'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "panel-cell-order": [
   "2487d3c1",
   "f43acdea",
   "1aede11d",
   "a8c37ffe"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
